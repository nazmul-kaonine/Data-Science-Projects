{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nazmul-kaonine/UTS_ML2019_ID13300912/blob/master/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tP7wrpTMvdll"
      },
      "source": [
        "# Draft and Experiment Area\n",
        "\n",
        "####Notes: \n",
        "1)Sirovich is spelled wrong in original paper\\\n",
        "2)All references are done using UTS Harvard style\\\n",
        "3)Mentionable websites: \\\n",
        "https://www.digitalvidya.com/blog/linear-discriminant-analysis/\\\n",
        "https://www.semanticscholar.org/paper/PCA-versus-LDA-Mart%C3%ADnez-Kak/cadb9a014a4c5bbec57aaf30391f472fa4b69b4d \\\n",
        "4)Important:a paper comparing classifiers on the basis of their accuracy on a training set would be of poor quality because generally the accuracy measured on a training set is higher than the actual test error. Other indicators of poor quality might be that the results in the paper could not be replicated by someone reading the paper (because they were not described clearly enough); a comparison was made between two things but the two things couldn’t fairly be compared; or conclusions were drawn from too few experiments  \n",
        "5) Application & uses \\\n",
        "6) Required: Depth and complexity of research 80 1, \\\n",
        "2 B.6 Accuracy of spelling, \\\n",
        "grammar and punctuation \\\n",
        "\n",
        "####An algorithm is developed based on Fisher\n",
        "This paper compares and contrasts different methods in the purpose of developing this algorithm.\n",
        "\n",
        "####Highlights \n",
        "• As above, PCA/LDA represents another aspect of improving “to refer to the seen (training) data when facing unseens (test)”. Such methods look for a transformation of the data into some space, in which the distance is more “meaningful”, i.e. facilitates solving the prediction task.  • Further Reading • Note LDA dates back to a classic work by Fisher in 1936, you may also review that paper. (Caveat: symbols/denotations could look obscure.) • Martinez and Kak did a critical study of the area in 2000. • In a broader sense, data representation learning is literally the core of machine learning. The searching for good representations leads to modern deep learning.\n",
        "\n",
        "####Criteria \n",
        " What attracts you. \\\n",
        " • Setting up the task: input and output \\\n",
        " • Motivation and background \\\n",
        " • The Method/Technique (you can be forgiven is making mistakes here) \\ \n",
        " • Conclusion (Taking home points). \\\n",
        " • Presentation \\\n",
        " • Your own comments on each part. \\\n",
        "\n",
        "####Word count \n",
        "Content: 300  \\\n",
        "Innovation: 300 \\\n",
        "Technical Quality: 200 \\\n",
        "Application and X-Factor: 200 \\\n",
        "Presentation: 100 \\\n",
        "\n",
        "####Questions & Short Answers\n",
        "First impression\n",
        "What is my chosen paper to read? \\\n",
        "A. \"Eigenfaces vs. Fisherfaces\": Recognition Using Class Specific Linear Projection\n",
        " \n",
        "What type of the main contribution the paper has made? \\\n",
        "A. Developed a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression.\n",
        " \n",
        "A theory or proposition (revealing something, from unknown to known)\n",
        "A method or algorithm (inventing a technique, from undoable to doable) \\\n",
        "A. Algorithm\n",
        "\n",
        "Before reading the main body of the paper, write down your first impression obtained from its abstract and short introduction. \\\n",
        "A.First impression: \n",
        "Talks about a broadly used topic in the modern era i.e. face recognition.\n",
        "\n",
        "Talks about developing algorithms and goes in depth on how to model transformations during face recognition regardless of the pose or lighting. This attracted me to read more.\n",
        "Easy to understand.\n",
        "\n",
        "Why does the paper attract you, such as, how it surprised you? Why do you think it addresses an important topic that will be helpful in your future study of machine learning? \\\n",
        "\n",
        "A. Something that my phone still struggles is to read my face in low light. This paper develops a solution to my real-life problem as which is actually a major challenge faced by companies like Apple, Google and alls lens-based technology at present. In computer vision, this is a hot topic. The idea is how to train computers to see like humans which is still an aspect of the future. Through machine learning I hope to be a part of this change and want to contribute in developing effective algorithms to detect faces under any circumstances.\n",
        "\n",
        "Read the paper abstract and introduction, list here all the notions that you don't know the precise meaning. If you think you have completed your list, compare the list with people around you who have chosen the same or a similar paper.\n",
        "(During the next 7 days) Re-consider the central problem of the paper \\\n",
        "\n",
        "A. Unknowns:\n",
        "polar quantization\n",
        "Murase and Nayar's appearance manifolds \\\n",
        "Eigenface technique (resolved later) \\\n",
        "Fisher's Linear Discriminant (resolved later) \\\n",
        "Principal components analysis (resolved later) \\\n",
        "\n",
        "####Initial Problems:\n",
        "Training test set error \\\n",
        "insensitive to pose \\\n",
        "Faces are not truly lambertian \\\n",
        "leaving-one-out \\\n",
        "small database \\\n",
        "\n",
        "\n",
        "\n",
        "# Review Report on \"Eigenfaces vs. Fisherfaces\": Recognition Using Class Specific Linear Projection\n",
        "\n",
        "## Introduction\n",
        "Facial Recognition technology have proved to be a favorite in this technological era. Although technology seems to be everchangning, facial recognition and analysis have proved to be crucial in making life easier. Therefore, just like every other emerging technology, this demands more research and analysis. To be precise, lighting, pose and facial expression are challenging to overcome. Research in this field has been missing until Belhumeur, Hespanha and Kriegman (1997) examined and evaluated four different methods and developed an algorithm called the \"Fisherface\" method to overcome the challenges related to variations in lighting. A successful attempt to this has been made on a small sized database, however, this algorithm does not address the variations in pose (Beymer 1994) and no attempt has been made on larger databases. The methods are derived from certain dimensionality reduction techniques such as Fisher's Linear Discriminant (FLD) and Principal Component Analysis (PCA) (Sirovich & Kirby 1987). In this paper the use of FLD  has produced results that are remarkable. This is widely believed as a general scenario but Martinez and Kak (2000) have showed that this is not always the case specially in small training data sets (Martínez, A.M. & Kak 2001). While FLD was developed in 1936 by Ronald A. Fisher, the original paper references to problems that were made of only two classes. It was not until 1947 that C.R.Rao applied it to multi-class problems. Today, they are both known as Linear Discriminant Analysis (LDA)(. These methods come in use by transforming inputs into some data space so that the output data is made more insightful. The goal is to assist the prediction on the test data after reading the training data and observing the distances inside the transformed data space. This paper explores and evaluates four such methods with experiments under different circumstances and carefully presents distinct conclusions to each method and their efficiency. \n",
        "\n",
        "## Content\n",
        "\n",
        "The significance of this research addresses the fact that under different lightning and pose variations (Beymer 1994), human faces can appear to be significantly different. Moreover, lighting variance has multiple counterparts such as intensity, direction and number of sources. The research is based on the following observations. All images taken of a Lambertian surface (surfaces that do not illuminate from within) under different lighting conditions and from a fixed viewpoint lie in a 3D linear subspace of the high dimensional image space (Shashua 1992). Secondly, facial pattern recognition can prove to be difficult due to shadowing and irregularities of certain regions of the face. Therefore, the authors conduct their research to find out a way that overcomes the variations in lighting and pose. These two techniques have been clearly defined in order for the reader to understand the method analysis that comes afterwards. Four pattern classification techniques are used on a certain problem. A set of labeled images have been provided and the methods are required to identify the same people from an unlabeled set. It is mentionable that all of these techniques are widely used in facial recognition literature. The first method addressed is correlation which is a simple nearest neighbor classifier. Eigenfaces which is about dimensionality reduction is based on PCA and is commonly used in computer vision (Sirovich & Kirby 1987). Thirdly, linear subspace method has been clearly described as recognition technique where three or more images of the surface is required and the shortest distance to each linear subspace from the new image is chosen for identification. Lastly, Fisherfaces is a class-specific dimensionality reduction technique (Fisher 1936) which has been shown to be effective in different lighting. The techniques have been discussed elaborately in terms of advantages and disadvantages, mathematics, uses, variations, results and remarks respectively.\n",
        "\n",
        "\n",
        "##Innovation\n",
        "During Fisher's time, predictions were done on classification of things or organisms. Fisher's technique that was established way back in 1936 is a class-based algorithm that was made for taxonomic classification (Fisher 1936). Afterwards, Cheng et al. proposed a method using Fisher's technique for facial recognition that was achieved by polar quantization of the shape. Based on a two class linear discriminant, a way of pattern rejection was developed by Baker and Nayar. But a very creative work was accomplished to read hand gestures by Cui et. al by following Fisher's Discriminant. Although, it was not done on facial recognition, it has been suggested that this method can be used to recognise faces under variable lighting. Not only this, this was given a different term calling it the Most Discriminating Factor (MDF) (Cui, Swets & Yeng 1995). The authors of this paper have followed this timeline, although similar to Cui et. al, have applied the same recognition technique to read 330 images from Harvard Database (Hallinan 1995) and 160 images from a database created by Yale and thus futher innovating this area of research. Corelation methods can be computationaly expensive and require more storage while Eigenfaces can result in the loss of discrimnatory data. While both of these methods are sensitive to lighting variation, Linear subspaces require ideal conditions to achieve error-free results. Therefore, Fisherfaces, a technique that uses linear separation for dimensionality reduction has proved to be insensitive to lighting variations. \n",
        "\n",
        "##Technical quality\n",
        "\n",
        "\n",
        "The technical presentation of how one method overshadows the other by explaining their drawbacks in this paper is praiseworthy. Following from above, PCA yields projection directions that maximize the total scatter across all classes whereas FLD maximizes the ratio of between-class scatter to that of within-class scatter. In PCA, the projection Wopt maximizes the total scatter matrix of the projected samples i.e. Wopt=arg max|WᵀSₜW| where Wᵀ is the scatter of the transformed vectors, Sₜ is total scatter matrix and W is a matrix with orthonormal columns. This idea has been used in Eigenfaces (Sirovich & Kirby 1987) but not sufficient exploration has been done for the reader to observe correlation. On the other hand, satisfactory results from Linear subspaces could be achieved if there was no shadowing. This adds up to another fact that faces are not truly Lambertian surfaces and thus the drawbacks to this method are many. Not only this, succint mathematical comparison has been made between the methods in visual graphs and the two best basis algorithms PCA and FLD have been compared separately. The authors supported their decision of developing Fisherfaces using many measures where the method tries to \"shape\" the scatter by selecting W in a way that the ratio of the between-class scatter and the within-class scatter is maximized. Although this technique has also been proposed to be ideal for variations in pose (Beymer 1994), no attempt has been made to cater for large variations in facial expressions. Another insight is that this paper compares classifiers on the basis of their accuracy on a training set which is unsatisfactory since, generally, the accuracy measured on a training set is higher than the actual test error. Similarly, \"leaving-one-out\" strategy (Duda & Hart 1973) is not a satisfactory mean of experimentation when dealing with something as vast as the number of faces in the world. \n",
        "\n",
        "##Application and X-factor¶ \n",
        "\n",
        "I found this paper to be of significant importance not only because of the high difference in the error rates but also how realistically this technique can be further used to build error-free algorithms in the future.\n",
        "Firstly, 330 images of five people from the Harvard database (Hallinan 1994) were divided into five subsets in accordance to various lighting conditions. The subsets contained a mixture of different faces and the four algorithms were tested. It was seen that Fisherfaces had no error rates for the first two subsets and the other three alorithms had a significant amount of error in recognizing the faces. All experiments were done using the nearest neighbor classifier technique and two types of experiments were done: extrapolation and interpolation. It has been also witnessed that Fisherfaces required less computational time.\n",
        "The next experiment was done on sixteen subjects on the Yale database. They contained images of people with different expressions, with and without glasses and facial hair. The images were used in to scales: one that is closely cropped and the other of the full face. Prior to this experiment, Fisherface and Eigenface methods (Sirovish & Kirby 1987) were compared to observe the number of principal components that yeilds the lowest error rates. Since the Fisherface method discounts the portions of the face that are of no use to recognise the individual, it  had significant less error rate than its competitors and a dramatic win considering the full face scale.\n",
        "Lastly, Fisherface method and PCA were compared to check the error rates due to glass recognition. Yet again, Fisherface gave an error rate of 5.3% (reduced space by 1) whereas PCA yielded 52.6% (reduced space by 10).\n",
        "What sets this research experiments apart is how the author visualised and remarked a wide number of real-life circumstances for this research and has successfully developed the Fisherface algorithm and leaving space for further research to be done on all possible illumination conditions.\n",
        "\n",
        "##Presentation\n",
        "\n",
        "The paper was fairly easy to read. On the contrary, it is notable that the correlation method needed deeper and more elaborate discussion as its other competitors. The structure of the paper was ordered in a way that the reader can understand the concepts first and gradually be leaded towards experimentation and results. Although the experiments were done on a wide range of conditions, a few number of experiments were done. For example, how natural light changes during different times of the day can prove to be troublesome while applying this technique. Furthermore, no remarks have been made on the future uses of this algorithm. The authors have clearly stated that the current method will likely break down in extreme lighting conditions but no pathway to its solution have been discussed. In addition to this, how this algorithm will function under modern day facial ornaments and tattoos still remains a question along with how this winning algorithm will perform on larger databases.\n",
        "\n",
        "\n",
        "##References\n",
        "Baker, S. & Nayar, S.K. 1996, 'Algorithms for pattern rejection', vol. 2, IEEE, pp. 869-74.\\\n",
        "Belhumeur, P.N., Hespanha, J.P. & Kriegman, D.J. 1996, 'Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection', Springer, pp. 43-58.\\\n",
        "Beymer, D. 1994, 'Face recognition under varying pose,\"', vol. 2, pp. 837-42.\\\n",
        "Cheng, Y.-Q., Liu, K., Yang, J., Zhuang, Y.-M. & Gu, N.-C. 1992, 'Human face recognition method based on the statistical model of small sample size', vol. 1607, International Society for Optics and Photonics, pp. 85-95.\\\n",
        "Cui, Y., Swets, D.L. & Weng, J.J. 1995, 'Learning-based hand sign recognition using SHOSLIF-M', IEEE, pp. 631-6.\\\n",
        "Duda, R.O. & Hart, P.E. 1973, 'Pattern recognition and scene analysis', Series Pattern recognition and scene analysis Wiley, New York.\\\n",
        "Fisher, R.A. 1936, '138: The Use of Multiple Measurements in Taxonomic Problems'.\\\n",
        "Hallinan, P. 1995, 'A deformable model for face recognition under arbitrary lighting conditions', PhD thesis, Harvard University.\\\n",
        "Martínez, A.M. & Kak, A.C. 2001, 'Pca versus lda', IEEE transactions on pattern analysis and machine intelligence, vol. 23, no. 2, pp. 228-33.\\\n",
        "Mehta, A. 2019, linear-discriminant-analysis, Everything You Need to Know About Linear Discriminant Analysis, Digital Vidya, Place of Publication, 22nd Aug 2019, <https://www.digitalvidya.com/blog/linear-discriminant-analysis/>.\\\n",
        "Shashua, A. 1992, 'Geometry and photometry in 3D visual recognition'.\\\n",
        "Sirovich, L. & Kirby, M. 1987, 'Low-dimensional procedure for the characterization of human faces', Journal of the Optical Society of America A, vol. 4, no. 3, pp. 519-24.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOTne9pB2cRa",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}