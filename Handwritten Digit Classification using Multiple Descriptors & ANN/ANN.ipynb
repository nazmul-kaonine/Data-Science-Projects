{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nazmul-kaonine/Data-Science-Projects/blob/master/Handwritten%20Digit%20Classification%20using%20Multiple%20Descriptors%20%26%20ANN/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGsqwhylaNZt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "0ab79b1a-c749-4a4d-d598-8ef3881116e3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import feature # This pacakge is used for LBP feature extraction\n",
        "from sklearn import svm # This pacakge is used for svm classification\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "import os\n",
        "import seaborn as sns # This pacakge is used for better visualization of data (e.g confusion matrix)\n",
        "import random\n",
        "\n",
        "# importing a module for splitting a dataset into train, and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import Knn classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# confusion metric\n",
        "from sklearn import metrics\n",
        "import math, numpy as np\n",
        "import sklearn.datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import glob\n",
        "import cv2\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LaJ1y4Gp1pA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "394316c0-fa0e-4247-b11f-5a1a8cd46493"
      },
      "source": [
        "    os.system('wget -N http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz')\n",
        "    os.system('wget -N http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz')\n",
        "    os.system('wget -N http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz')\n",
        "    os.system('wget -N http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GvcFJe1aiRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mnist(path, kind='train'):\n",
        "    import os\n",
        "    import gzip\n",
        "    import numpy as np\n",
        "\n",
        "    \"\"\"Load MNIST data from `path`\"\"\"\n",
        "    labels_path = os.path.join(path,\n",
        "                               '%s-labels-idx1-ubyte.gz'\n",
        "                               % kind)\n",
        "    images_path = os.path.join(path,\n",
        "                               '%s-images-idx3-ubyte.gz'\n",
        "                               % kind)\n",
        "\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                               offset=8)\n",
        "\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                               offset=16).reshape(len(labels), 784)\n",
        "                                \n",
        "                              \n",
        "\n",
        "\n",
        "                        \n",
        "        \n",
        "\n",
        "\n",
        "    return images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92y6kmkpkRDp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "ddcd4f31-2ab8-4f19-fae4-211b64e5188f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZYln6SWxP5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = load_mnist('/content/gdrive/My Drive/DLA2/', kind='train')\n",
        "X_test, y_test = load_mnist('/content/gdrive/My Drive/DLA2/', kind='t10k')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tiva_NdsrYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "994f271d-980a-475b-ae57-df65642aaed7"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiObXk8Hp4Uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_image(position):\n",
        "    image = X_train[position].squeeze()\n",
        "    plt.title('Example %d. Label: %d' % (position, y_train[position]))\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94jwYzICttin",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "464e6ed8-ea82-4c1c-db61-b9f4eb7892ca"
      },
      "source": [
        "display_image(0)\n",
        "plt.show()\n",
        "display_image(59999)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzdvR-zt80Ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=X_train.reshape(-1,784)\n",
        "X_test=X_test.reshape(-1,784)\n",
        "\n",
        "X_test.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IOZ4yM1QkDL",
        "colab_type": "text"
      },
      "source": [
        "#RAW INPUTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2gZ_Cz4QmMR",
        "colab_type": "text"
      },
      "source": [
        "####K NEAREST NEIGHBOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrDOvD9UT6kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GFkA-AzyvgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "97d5a597-60c5-45bf-ab02-1cf61eb54265"
      },
      "source": [
        "train_path=train_path.reshape(-1,28,28)\n",
        "X_test=X_test.reshape(-1,28,28)\n",
        "\n",
        "# print the size of the result reshaped train and test data splits\n",
        "\n",
        "print(\"Train dataset after reshaping:{}\".format(np.shape(X_train)))\n",
        "print(\"Test dataset after reshaping :{}\".format(np.shape(X_test)))\n",
        "## Show an image from theh dataset\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(train_path[4])\n",
        "print(train_labels[4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset after reshaping:(55000, 28, 28)\n",
            "Test dataset after reshaping :(10000, 28, 28)\n",
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-6GFQoorTwJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "a2c6268a-d646-416a-9c8d-dd10af8d2658"
      },
      "source": [
        "sns.countplot(train_data)\n",
        "plt.show()# looks kinda okay\n",
        "# or we can just print\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMQ598Lwrap_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "58d6242c-6d48-4cb5-8e3a-6280794d1f4b"
      },
      "source": [
        "print(\"train data\")\n",
        "y_value=np.zeros((1,10))\n",
        "for i in range (10):\n",
        "    print(\"occurance of \",i,\"=\",np.count_nonzero(train_labels==i))\n",
        "    y_value[0,i-1]= np.count_nonzero(train_labels==i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data\n",
            "occurance of  0 = 5923\n",
            "occurance of  1 = 6742\n",
            "occurance of  2 = 5958\n",
            "occurance of  3 = 6131\n",
            "occurance of  4 = 5842\n",
            "occurance of  5 = 5421\n",
            "occurance of  6 = 5918\n",
            "occurance of  7 = 6265\n",
            "occurance of  8 = 5851\n",
            "occurance of  9 = 5949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WscVbkhFrmTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "7b7ce3c7-6628-429c-8af0-49ba49465fc4"
      },
      "source": [
        "y_value=y_value.ravel()\n",
        "x_value=[0,1,2,3,4,5,6,7,8,9]\n",
        "plt.xlabel('label')\n",
        "plt.ylabel('count')\n",
        "plt.bar(x_value,y_value,0.7,color='g')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 10 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmPdt9VXtJyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = (X_train)\n",
        "test_data= (X_test)\n",
        "#separating labels and pixels\n",
        "train_labels=(y_train)\n",
        "\n",
        "#train_data=train_data/train_data.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Un54QH81H0f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12a1b5f6-c7cc-4f3a-f270-0bea7e800b52"
      },
      "source": [
        "train_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1Z2ea0L6PVk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f53b53f0-bdfc-48b2-ffcb-a081e1daec40"
      },
      "source": [
        "train_labels[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc3QkUxMzpZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "36b7c330-bc38-4043-8055-4c09d61d8ff7"
      },
      "source": [
        "#converting train_label in one hot encoder representation \n",
        "train_data=np.reshape(train_data,[784,60000])\n",
        "train_label=np.zeros((10,60000))\n",
        "for col in range (60000):\n",
        "    val=train_labels[col]\n",
        "    for row in range (10):\n",
        "        if (val==row):\n",
        "            train_label[val,col]=1\n",
        "print(\"train_data shape=\"+str(np.shape(train_data)))\n",
        "print(\"train_label shape=\"+str(np.shape(train_label)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_data shape=(784, 60000)\n",
            "train_label shape=(10, 60000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4mXRgFIuEW3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ea9dde0-d6c6-4ada-8fe0-75d006861fe6"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 60000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYtZ_3Qb7y9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#activation functions sigmoid relu and softmax\n",
        "def sigmoid(Z):\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)    \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "def softmax(Z):\n",
        "    e_x = np.exp(Z)\n",
        "    A= e_x / np.sum(np.exp(Z))  \n",
        "    cache=Z\n",
        "    return A,cache   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWmJ3aOm74x5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#derivative of activation function\n",
        "def relu_backward(dA, cache):\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    dZ[Z <= 0] = 0\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):    \n",
        "    Z = cache\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    return dZ\n",
        "\n",
        "def softmax_backward(Z,cache):\n",
        "    Z=cache\n",
        "    length=10  \n",
        "    dZ=np.zeros((60000,10))\n",
        "    Z=np.transpose(Z)\n",
        "    for row in range (0,60000):\n",
        "            den=(np.sum(np.exp(Z[row,:])))*(np.sum(np.exp(Z[row,:])))\n",
        "            for col in range (0,10):\n",
        "                sums=0\n",
        "                for j in range (0,10):\n",
        "                    if (j!=col):\n",
        "                        sums=sums+(math.exp(Z[row,j]))\n",
        "                \n",
        "                dZ[row,col]=(math.exp(Z[row,col])*sums)/den           \n",
        "    dZ=np.transpose(dZ)\n",
        "    Z=np.transpose(Z)\n",
        "\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    return dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUWnhzq-79Jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initializing the parameters weights and bias\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    #np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "    \n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtKqhsjN8AQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#forward propagation\n",
        "def linear_forward(A, W, b):\n",
        "    Z = np.dot(W,A) +b\n",
        "    cache = (A, W, b)\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    return Z, cache\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        #print(\"Z=\"+str(Z))\n",
        "        A, activation_cache = relu(Z) \n",
        "    elif activation == \"softmax\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = softmax(Z)\n",
        "    cache = (linear_cache, activation_cache)\n",
        "    return A, cache\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"softmax\")\n",
        "    caches.append(cache)               \n",
        "    return AL, caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP9kwfgT8C3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cost function\n",
        "def compute_cost(AL, Y):\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n",
        "    #print(\"cost=\"+str(cost))\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yvkyc8i8GhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiVlNeJN8KC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#upgrade function for weights and bias\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    for l in range(len_update-1):\n",
        "        parameters[\"W\" + str(l+1)] =parameters[\"W\" + str(l+1)] - (learning_rate*grads[\"dW\" + str(l+1)])\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate*grads[\"db\" + str(l+1)])\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvcwIU8T8Mx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_graph(cost_plot):\n",
        "       \n",
        "    x_value=list(range(1,len(cost_plot)+1))\n",
        "    #print(x_value)\n",
        "    #print(cost_plot)\n",
        "    plt.xlabel('iteration')\n",
        "    plt.ylabel('cost')\n",
        "    plt.plot(x_value,cost_plot,0.,color='g')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbXG2GZE8QrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining structure of neural network\n",
        "layers_dims = [784,500,400,300,100,10] #  n-layer model (n=6 including input and output layer)\n",
        "len_update=len(layers_dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRCyq9HL8US6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to call sub_functions\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate , num_iterations , print_cost=False):#lr was 0.009\n",
        "    print(\"training...\")\n",
        "    costs = []  \n",
        "    cost_plot=np.zeros(num_iterations)\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    for i in range(0, num_iterations):\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        cost =compute_cost(AL, Y)\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate) \n",
        "        cost_plot[i]=cost;\n",
        "    \n",
        "    plot_graph(cost_plot)\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYgVee4b8Xly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "317d4f0c-b194-4c06-828c-4aa1ceb787d8"
      },
      "source": [
        "#variable parameter in network learning_rate, iterationd \n",
        "parameters = L_layer_model(train_data, train_label, layers_dims,learning_rate = 0.0005, num_iterations =1 , print_cost = True) \n",
        "print(\"training done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-9915b494d5e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-b938b7b7c311>\u001b[0m in \u001b[0;36mL_layer_model\u001b[0;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_parameters_deep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-f0faa572642b>\u001b[0m in \u001b[0;36mL_model_forward\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-f0faa572642b>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[0;34m(A_prev, W, b, activation)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlinear_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1k2nYdhPrrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = NN(num_inputs,hidden_size,num_outputs)\n",
        "cost_dict, tests_dict = model.train(x_train,y_train,num_iterations=num_iterations,learning_rate=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S38OCT7M9FhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "#data file type h5py\n",
        "import time\n",
        "import copy\n",
        "from random import randint\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOLISPWG9G46",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6c803b9-a402-434a-d3b0-3e4b3559df6e"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqwHPQiG9xlb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "492e95ad-4022-4c79-885f-0645574f515b"
      },
      "source": [
        "X_train=X_train.reshape(-1,28,28)\n",
        "X_test=X_test.reshape(-1,28,28)\n",
        "\n",
        "# print the size of the result reshaped train and test data splits\n",
        "\n",
        "print(\"Train dataset after reshaping:{}\".format(np.shape(X_train)))\n",
        "print(\"Test dataset after reshaping :{}\".format(np.shape(X_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset after reshaping:(60000, 28, 28)\n",
            "Test dataset after reshaping :(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqzpzPqnQ_GD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "####################################################################################\n",
        "#Implementation of stochastic gradient descent algorithm\n",
        "\n",
        "class NN:\n",
        "    first_layer = {}\n",
        "    second_layer = {}\n",
        "\n",
        "    def __init__(self, inputs, hidden, outputs):\n",
        "        # initialize the model parameters, including the first and second layer \n",
        "        # parameters and biases\n",
        "        self.first_layer['para'] = np.random.randn(hidden,inputs) / np.sqrt(num_inputs)\n",
        "        self.first_layer['bias'] = np.random.randn(hidden,1) / np.sqrt(hidden)\n",
        "        self.second_layer['para'] = np.random.randn(outputs,hidden) / np.sqrt(hidden)\n",
        "        self.second_layer['bias'] = np.random.randn(outputs,1) / np.sqrt(hidden)\n",
        "        self.input_size = inputs\n",
        "        self.hid_size = hidden\n",
        "        self.output_size = outputs\n",
        "\n",
        "    def __activfunc(self,Z,type = 'ReLU',deri = False):\n",
        "        # implement the activation function\n",
        "        if type == 'ReLU':\n",
        "            if deri == True:\n",
        "                return np.array([1 if i>0 else 0 for i in np.squeeze(Z)])\n",
        "            else:\n",
        "                return np.array([i if i>0 else 0 for i in np.squeeze(Z)])\n",
        "        elif type == 'Sigmoid':\n",
        "            if deri == True:\n",
        "                return 1/(1+np.exp(-Z))*(1-1/(1+np.exp(-Z)))\n",
        "            else:\n",
        "                return 1/(1+np.exp(-Z))\n",
        "        elif type == 'tanh':\n",
        "            if deri == True:\n",
        "                return \n",
        "            else:\n",
        "                return 1-(np.tanh(Z))**2\n",
        "        else:\n",
        "            raise TypeError('Invalid type!')\n",
        "\n",
        "    def __Softmax(self,z):\n",
        "        # implement the softmax function\n",
        "        return 1/sum(np.exp(z)) * np.exp(z)\n",
        "\n",
        "    def __cross_entropy_error(self,v,y):\n",
        "        # implement the cross entropy error\n",
        "        return -np.log(v[y])\n",
        "\n",
        "    def __forward(self,x,y):\n",
        "        # implement the forward computation, calculation of prediction list and error\n",
        "        Z = np.matmul(self.first_layer['para'],x).reshape((self.hid_size,1)) + self.first_layer['bias']\n",
        "        H = np.array(self.__activfunc(Z)).reshape((self.hid_size,1))\n",
        "        U = np.matmul(self.second_layer['para'],H).reshape((self.output_size,1)) + self.second_layer['bias']\n",
        "        predict_list = np.squeeze(self.__Softmax(U))\n",
        "        error = self.__cross_entropy_error(predict_list,y)\n",
        "        \n",
        "        dic = {\n",
        "            'Z':Z,\n",
        "            'H':H,\n",
        "            'U':U,\n",
        "            'f_X':predict_list.reshape((1,self.output_size)),\n",
        "            'error':error\n",
        "        }\n",
        "        return dic\n",
        "\n",
        "    def __back_propagation(self,x,y,f_result):\n",
        "        # implement the back propagation process, compute the gradients\n",
        "        E = np.array([0]*self.output_size).reshape((1,self.output_size))\n",
        "        E[0][y] = 1\n",
        "        dU = (-(E - f_result['f_X'])).reshape((self.output_size,1))\n",
        "        db_2 = copy.copy(dU)\n",
        "        dC = np.matmul(dU,f_result['H'].transpose())\n",
        "        delta = np.matmul(self.second_layer['para'].transpose(),dU)\n",
        "        db_1 = delta.reshape(self.hid_size,1)*self.__activfunc(f_result['Z'],deri=True).reshape(self.hid_size,1)\n",
        "        dW = np.matmul(db_1.reshape((self.hid_size,1)),x.reshape((1,784)))\n",
        "\n",
        "        grad = {\n",
        "            'dC':dC,\n",
        "            'db_2':db_2,\n",
        "            'db_1':db_1,\n",
        "            'dW':dW\n",
        "        }\n",
        "        return grad\n",
        "\n",
        "    def __optimize(self,b_result, learning_rate):\n",
        "        # update the hyperparameters\n",
        "        self.second_layer['para'] -= learning_rate*b_result['dC']\n",
        "        self.second_layer['bias'] -= learning_rate*b_result['db_2']\n",
        "        self.first_layer['bias'] -= learning_rate*b_result['db_1']\n",
        "        self.first_layer['para'] -= learning_rate*b_result['dW']\n",
        "\n",
        "    def __loss(self,X_train,Y_train):\n",
        "        # implement the loss function of the training set\n",
        "        loss = 0\n",
        "        for n in range(len(X_train)):\n",
        "            y = Y_train[n]\n",
        "            x = X_train[n][:]\n",
        "            loss += self.__forward(x,y)['error']\n",
        "        return loss\n",
        "\n",
        "    def train(self, X_train, Y_train, num_iterations = 1000, learning_rate = 0.5):\n",
        "        # generate a random list of indices for the training set\n",
        "        rand_indices = np.random.choice(len(X_train), num_iterations, replace=True)\n",
        "        \n",
        "        def l_rate(base_rate, ite, num_iterations, schedule = False):\n",
        "        # determine whether to use the learning schedule\n",
        "            if schedule == True:\n",
        "                return base_rate * 10 ** (-np.floor(ite/num_iterations*5))\n",
        "            else:\n",
        "                return base_rate\n",
        "\n",
        "        count = 1\n",
        "        loss_dict = {}\n",
        "        test_dict = {}\n",
        "\n",
        "        for i in rand_indices:\n",
        "            f_result = self.__forward(X_train[i],Y_train[i])\n",
        "            b_result = self.__back_propagation(X_train[i],Y_train[i],f_result)\n",
        "            self.__optimize(b_result,l_rate(learning_rate,i,num_iterations,True))\n",
        "            \n",
        "            if count % 1000 == 0:\n",
        "                if count % 5000 == 0:\n",
        "                    loss = self.__loss(X_train,Y_train)\n",
        "                    test = self.testing(x_test,y_test)\n",
        "                    print('Trained for {} times,'.format(count),'loss = {}, test = {}'.format(loss,test))\n",
        "                    loss_dict[str(count)]=loss\n",
        "                    test_dict[str(count)]=test\n",
        "                else:\n",
        "                    print('Trained for {} times,'.format(count))\n",
        "            count += 1\n",
        "\n",
        "        print('Training finished!')\n",
        "        return loss_dict, test_dict\n",
        "\n",
        "    def testing(self,X_test, Y_test):\n",
        "        # test the model on the training dataset\n",
        "        total_correct = 0\n",
        "        for n in range(len(X_test)):\n",
        "            y = Y_test[n]\n",
        "            x = X_test[n][:]\n",
        "            prediction = np.argmax(self.__forward(x,y)['f_X'])\n",
        "            if (prediction == y):\n",
        "                total_correct += 1\n",
        "        print('Accuarcy Test: ',total_correct/len(X_test))\n",
        "        return total_correct/np.float(len(X_test))\n",
        "\n",
        "####################################################################################\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3y9HkYQ92cf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6f8525e-92ff-4670-971f-b7aa5b2ad284"
      },
      "source": [
        "\n",
        "x_train = X_train\n",
        "y_train = y_train\n",
        "x_test = X_test\n",
        "y_test = y_test\n",
        "\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuE5Uye29UqE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ad6f374-cf0f-4dce-d371-238745a78241"
      },
      "source": [
        "# set the number of iterations\n",
        "num_iterations = 200000\n",
        "# set the base learning rate\n",
        "learning_rate = 0.01\n",
        "# number of inputs\n",
        "num_inputs = 784\n",
        "# number of outputs\n",
        "num_outputs = 10\n",
        "# size of hidden layer\n",
        "hidden_size = 300\n",
        "\n",
        "# data fitting, training and accuracy evaluation\n",
        "model = NN(num_inputs,hidden_size,num_outputs)\n",
        "cost_dict, tests_dict = model.train(x_train,y_train,num_iterations=num_iterations,learning_rate=learning_rate)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in multiply\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Trained for 1000 times,\n",
            "Trained for 2000 times,\n",
            "Trained for 3000 times,\n",
            "Trained for 4000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 5000 times, loss = nan, test = 0.098\n",
            "Trained for 6000 times,\n",
            "Trained for 7000 times,\n",
            "Trained for 8000 times,\n",
            "Trained for 9000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 10000 times, loss = nan, test = 0.098\n",
            "Trained for 11000 times,\n",
            "Trained for 12000 times,\n",
            "Trained for 13000 times,\n",
            "Trained for 14000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 15000 times, loss = nan, test = 0.098\n",
            "Trained for 16000 times,\n",
            "Trained for 17000 times,\n",
            "Trained for 18000 times,\n",
            "Trained for 19000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 20000 times, loss = nan, test = 0.098\n",
            "Trained for 21000 times,\n",
            "Trained for 22000 times,\n",
            "Trained for 23000 times,\n",
            "Trained for 24000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 25000 times, loss = nan, test = 0.098\n",
            "Trained for 26000 times,\n",
            "Trained for 27000 times,\n",
            "Trained for 28000 times,\n",
            "Trained for 29000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 30000 times, loss = nan, test = 0.098\n",
            "Trained for 31000 times,\n",
            "Trained for 32000 times,\n",
            "Trained for 33000 times,\n",
            "Trained for 34000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 35000 times, loss = nan, test = 0.098\n",
            "Trained for 36000 times,\n",
            "Trained for 37000 times,\n",
            "Trained for 38000 times,\n",
            "Trained for 39000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 40000 times, loss = nan, test = 0.098\n",
            "Trained for 41000 times,\n",
            "Trained for 42000 times,\n",
            "Trained for 43000 times,\n",
            "Trained for 44000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 45000 times, loss = nan, test = 0.098\n",
            "Trained for 46000 times,\n",
            "Trained for 47000 times,\n",
            "Trained for 48000 times,\n",
            "Trained for 49000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 50000 times, loss = nan, test = 0.098\n",
            "Trained for 51000 times,\n",
            "Trained for 52000 times,\n",
            "Trained for 53000 times,\n",
            "Trained for 54000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 55000 times, loss = nan, test = 0.098\n",
            "Trained for 56000 times,\n",
            "Trained for 57000 times,\n",
            "Trained for 58000 times,\n",
            "Trained for 59000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 60000 times, loss = nan, test = 0.098\n",
            "Trained for 61000 times,\n",
            "Trained for 62000 times,\n",
            "Trained for 63000 times,\n",
            "Trained for 64000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 65000 times, loss = nan, test = 0.098\n",
            "Trained for 66000 times,\n",
            "Trained for 67000 times,\n",
            "Trained for 68000 times,\n",
            "Trained for 69000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 70000 times, loss = nan, test = 0.098\n",
            "Trained for 71000 times,\n",
            "Trained for 72000 times,\n",
            "Trained for 73000 times,\n",
            "Trained for 74000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 75000 times, loss = nan, test = 0.098\n",
            "Trained for 76000 times,\n",
            "Trained for 77000 times,\n",
            "Trained for 78000 times,\n",
            "Trained for 79000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 80000 times, loss = nan, test = 0.098\n",
            "Trained for 81000 times,\n",
            "Trained for 82000 times,\n",
            "Trained for 83000 times,\n",
            "Trained for 84000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 85000 times, loss = nan, test = 0.098\n",
            "Trained for 86000 times,\n",
            "Trained for 87000 times,\n",
            "Trained for 88000 times,\n",
            "Trained for 89000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 90000 times, loss = nan, test = 0.098\n",
            "Trained for 91000 times,\n",
            "Trained for 92000 times,\n",
            "Trained for 93000 times,\n",
            "Trained for 94000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 95000 times, loss = nan, test = 0.098\n",
            "Trained for 96000 times,\n",
            "Trained for 97000 times,\n",
            "Trained for 98000 times,\n",
            "Trained for 99000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 100000 times, loss = nan, test = 0.098\n",
            "Trained for 101000 times,\n",
            "Trained for 102000 times,\n",
            "Trained for 103000 times,\n",
            "Trained for 104000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 105000 times, loss = nan, test = 0.098\n",
            "Trained for 106000 times,\n",
            "Trained for 107000 times,\n",
            "Trained for 108000 times,\n",
            "Trained for 109000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 110000 times, loss = nan, test = 0.098\n",
            "Trained for 111000 times,\n",
            "Trained for 112000 times,\n",
            "Trained for 113000 times,\n",
            "Trained for 114000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 115000 times, loss = nan, test = 0.098\n",
            "Trained for 116000 times,\n",
            "Trained for 117000 times,\n",
            "Trained for 118000 times,\n",
            "Trained for 119000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 120000 times, loss = nan, test = 0.098\n",
            "Trained for 121000 times,\n",
            "Trained for 122000 times,\n",
            "Trained for 123000 times,\n",
            "Trained for 124000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 125000 times, loss = nan, test = 0.098\n",
            "Trained for 126000 times,\n",
            "Trained for 127000 times,\n",
            "Trained for 128000 times,\n",
            "Trained for 129000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 130000 times, loss = nan, test = 0.098\n",
            "Trained for 131000 times,\n",
            "Trained for 132000 times,\n",
            "Trained for 133000 times,\n",
            "Trained for 134000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 135000 times, loss = nan, test = 0.098\n",
            "Trained for 136000 times,\n",
            "Trained for 137000 times,\n",
            "Trained for 138000 times,\n",
            "Trained for 139000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 140000 times, loss = nan, test = 0.098\n",
            "Trained for 141000 times,\n",
            "Trained for 142000 times,\n",
            "Trained for 143000 times,\n",
            "Trained for 144000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 145000 times, loss = nan, test = 0.098\n",
            "Trained for 146000 times,\n",
            "Trained for 147000 times,\n",
            "Trained for 148000 times,\n",
            "Trained for 149000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 150000 times, loss = nan, test = 0.098\n",
            "Trained for 151000 times,\n",
            "Trained for 152000 times,\n",
            "Trained for 153000 times,\n",
            "Trained for 154000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 155000 times, loss = nan, test = 0.098\n",
            "Trained for 156000 times,\n",
            "Trained for 157000 times,\n",
            "Trained for 158000 times,\n",
            "Trained for 159000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 160000 times, loss = nan, test = 0.098\n",
            "Trained for 161000 times,\n",
            "Trained for 162000 times,\n",
            "Trained for 163000 times,\n",
            "Trained for 164000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 165000 times, loss = nan, test = 0.098\n",
            "Trained for 166000 times,\n",
            "Trained for 167000 times,\n",
            "Trained for 168000 times,\n",
            "Trained for 169000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 170000 times, loss = nan, test = 0.098\n",
            "Trained for 171000 times,\n",
            "Trained for 172000 times,\n",
            "Trained for 173000 times,\n",
            "Trained for 174000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 175000 times, loss = nan, test = 0.098\n",
            "Trained for 176000 times,\n",
            "Trained for 177000 times,\n",
            "Trained for 178000 times,\n",
            "Trained for 179000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 180000 times, loss = nan, test = 0.098\n",
            "Trained for 181000 times,\n",
            "Trained for 182000 times,\n",
            "Trained for 183000 times,\n",
            "Trained for 184000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 185000 times, loss = nan, test = 0.098\n",
            "Trained for 186000 times,\n",
            "Trained for 187000 times,\n",
            "Trained for 188000 times,\n",
            "Trained for 189000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 190000 times, loss = nan, test = 0.098\n",
            "Trained for 191000 times,\n",
            "Trained for 192000 times,\n",
            "Trained for 193000 times,\n",
            "Trained for 194000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 195000 times, loss = nan, test = 0.098\n",
            "Trained for 196000 times,\n",
            "Trained for 197000 times,\n",
            "Trained for 198000 times,\n",
            "Trained for 199000 times,\n",
            "Accuarcy Test:  0.098\n",
            "Trained for 200000 times, loss = nan, test = 0.098\n",
            "Training finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E75DMC3bBltw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "7ac8638d-d251-47bd-8f6e-00d50825306b"
      },
      "source": [
        "model.fit(data_train, labels_train)\n",
        "accu = model.testing(x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-140-ca8a2c2fd9e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NN' object has no attribute 'fit'"
          ]
        }
      ]
    }
  ]
}
